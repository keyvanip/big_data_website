[
  {
    "objectID": "ml-kmean.html",
    "href": "ml-kmean.html",
    "title": "Current Sentiment Clustering",
    "section": "",
    "text": "Code\n---\ntitle: \"Current Sentiment Clustering\"\nformat:\n  html:\n    embed-resources: true\n    code-fold: true\nexecute:\n  warning: false\n  message: false\n---\n\n\nIn the evolving landscape of the insurance industry, understanding customer behavior and preferences is paramount for enhancing service quality, managing risk, and optimizing business strategies. With the proliferation of data on customer interactions, claims, and policies, advanced analytical techniques have become essential for deriving actionable insights from large and complex datasets. This study leverages clustering and correlation analysis to uncover patterns in customer behavior, sentiment, and claim characteristics, aiming to segment the customer base and reveal insights that can guide targeted decision-making.\n\n\nCode\nimport pandas as pd\n\nfile_path = \"../../data/cleaned_data/current_first_half_merged.csv\"\ndata = pd.read_csv(file_path)\n\ndata.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nCustomer\nstate\nCustomer Lifetime Value\nResponse\nCoverage\nCoverage Index\nEducation\nEducation Index\nEmployment Status\n...\nSales Channel Index\nTotal Claim Amount\nVehicle Class\nVehicle Class Index\nVehicle Size\nVehicle Size Index\nmonth\nyear\navg_sentiment_score\navg_sentiment\n\n\n\n\n0\n1\nQC35222\nCalifornia\n3622.69\nNo\nBasic\n0\nBachelor\n2\nEmployed\n...\n0\n380.90\nFour-Door Car\n1\nMedsize\n1\n1\n2024\n-0.223594\nNegative\n\n\n1\n2\nAE98193\nWashington\n10610.21\nNo\nBasic\n0\nHigh School or Below\n0\nUnemployed\n...\n1\n1098.36\nSUV\n4\nMedsize\n1\n1\n2024\n-0.371933\nNegative\n\n\n2\n3\nTM23514\nOregon\n13868.02\nNo\nExtended\n1\nCollege\n1\nEmployed\n...\n0\n783.64\nSUV\n4\nMedsize\n1\n1\n2024\n0.078810\nPositive\n\n\n3\n4\nWB38524\nCalifornia\n4008.95\nNo\nBasic\n0\nHigh School or Below\n0\nEmployed\n...\n1\n479.52\nTwo-Door Car\n0\nSmall\n0\n1\n2024\n-0.223594\nNegative\n\n\n4\n5\nQZ42725\nWashington\n3119.69\nNo\nBasic\n0\nBachelor\n2\nUnemployed\n...\n2\n622.08\nFour-Door Car\n1\nMedsize\n1\n1\n2024\n-0.371933\nNegative\n\n\n\n\n5 rows × 38 columns\n\n\n\n\n\nCode\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nnumeric_data_1 = data.select_dtypes(include=['float64', 'int64'])\n\npearson_corr = numeric_data_1.corr(method='pearson')\n\nmask = np.triu(np.ones_like(pearson_corr, dtype=bool))\n\n# Create a heatmap with the mask applied\nplt.figure(figsize=(16, 14))\nsns.heatmap(pearson_corr, mask=mask, annot=True, cmap='coolwarm', fmt=\".2f\", cbar_kws={\"shrink\": .8})\nplt.title(\"Pearson Correlation Matrix (Lower Triangle)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThere’s a positive correlation between income and policy type (comprehensive policies) – wealthier customers tend to prefer more extensive coverage. On the other hand, negative correlation between sentiment score and claim amount – customers with more positive sentiments tend to file fewer claims.\n\n\nCode\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\n\n\n\n\nCode\nle = LabelEncoder()\ndata['state_encoded'] = le.fit_transform(data['state'])\n\n\n\n\nCode\ndata.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nCustomer\nstate\nCustomer Lifetime Value\nResponse\nCoverage\nCoverage Index\nEducation\nEducation Index\nEmployment Status\n...\nTotal Claim Amount\nVehicle Class\nVehicle Class Index\nVehicle Size\nVehicle Size Index\nmonth\nyear\navg_sentiment_score\navg_sentiment\nstate_encoded\n\n\n\n\n0\n1\nQC35222\nCalifornia\n3622.69\nNo\nBasic\n0\nBachelor\n2\nEmployed\n...\n380.90\nFour-Door Car\n1\nMedsize\n1\n1\n2024\n-0.223594\nNegative\n1\n\n\n1\n2\nAE98193\nWashington\n10610.21\nNo\nBasic\n0\nHigh School or Below\n0\nUnemployed\n...\n1098.36\nSUV\n4\nMedsize\n1\n1\n2024\n-0.371933\nNegative\n4\n\n\n2\n3\nTM23514\nOregon\n13868.02\nNo\nExtended\n1\nCollege\n1\nEmployed\n...\n783.64\nSUV\n4\nMedsize\n1\n1\n2024\n0.078810\nPositive\n3\n\n\n3\n4\nWB38524\nCalifornia\n4008.95\nNo\nBasic\n0\nHigh School or Below\n0\nEmployed\n...\n479.52\nTwo-Door Car\n0\nSmall\n0\n1\n2024\n-0.223594\nNegative\n1\n\n\n4\n5\nQZ42725\nWashington\n3119.69\nNo\nBasic\n0\nBachelor\n2\nUnemployed\n...\n622.08\nFour-Door Car\n1\nMedsize\n1\n1\n2024\n-0.371933\nNegative\n4\n\n\n\n\n5 rows × 39 columns\n\n\n\n\n\nCode\ndata_1 = data.select_dtypes(include=['float64', 'int64'])\n# Check standard deviation\nlow_variance_cols = data_1.std()[data_1.std() &lt; 0.1].index\n\n\n\n\nCode\nprint(\"Low variance columns:\", low_variance_cols)\n\n\nLow variance columns: Index(['year'], dtype='object')\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.manifold import TSNE\n\n\n\n\nCode\ndf_numeric = data_1.select_dtypes(include=['float64', 'int64'])\nscaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df_numeric)\n\n\n\n\nCode\nX = df_numeric.drop(['Unnamed: 0', 'avg_sentiment_score', 'year', 'state_encoded', 'month'], axis=1)\ny = data['avg_sentiment']\n\n\n\n\nCode\n# Encode categorical target variable\nle = LabelEncoder()\ny = le.fit_transform(y)\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Training set size: {X_train.shape}\")\nprint(f\"Test set size: {X_test.shape}\")\n\n\nTraining set size: (1470, 19)\nTest set size: (368, 19)\n\n\n\n\nCode\n# Initialize the model\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Train the model\nrf.fit(X_train, y_train)\n\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(random_state=42) \n\n\n\n\nCode\ny_pred = rf.predict(X_test)\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Feature importance from the model\nimportances = rf.feature_importances_\nfeatures = X.columns\n\n# Plot feature importance\nplt.figure(figsize=(10, 6))\nplt.barh(features, importances, color='red')\nplt.xlabel('Importance Score')\nplt.title('Feature Importance')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfor feature, importance in zip(features, importances):\n    print(f'{feature}: {importance}')\n\n\nCustomer Lifetime Value: 0.10488085128522988\nCoverage Index: 0.021341145274838724\nEducation Index: 0.04129707109778181\nEmployment Status Index: 0.023795706460716493\nIncome: 0.08934990199837156\nLocation Index: 0.020463934367078586\nMarital Status Index: 0.026364747430158608\nMonthly Premium Auto: 0.09245887603733369\nMonths Since Last Claim: 0.08907213126262188\nMonths Since Policy Inception: 0.10437893277094247\nNumber of Open Complaints: 0.02399823881895443\nNumber of Policies: 0.044530847297521686\nPolicy Type Index: 0.020903394925031585\nPolicy Index: 0.06207740206426565\nRenew Offer Type: 0.03510960952603887\nSales Channel Index: 0.038746061779391834\nTotal Claim Amount: 0.10656067488971306\nVehicle Class Index: 0.02867249319535218\nVehicle Size Index: 0.02599797951865706\n\n\nWe use models like random forest to assess feature importance to see features with high importance scores. When we are selecting the optimal features combination, be careful to exclude features that are highly correlated with each other, as they might introduce multicollinearity and redundancy. The feature importance plot shows how each attribute contributes to the prediction of cluster membership. Features with higher importance values have a more significant influence on determining the clusters.\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# Select relevant features for clustering\nfeatures = data[['Income', 'Policy Type', 'Number of Open Complaints', \n                 'Vehicle Class', 'Total Claim Amount', \n                 'avg_sentiment_score', 'avg_sentiment', 'Monthly Premium Auto']]\n\n# Encode categorical features (e.g., avg_sentiment, Policy Type, Vehicle Class)\nlabel_encoders = {}\ncategorical_columns = ['Policy Type', 'Vehicle Class', 'avg_sentiment']\n\nfor col in categorical_columns:\n    le = LabelEncoder()\n    features[col] = le.fit_transform(features[col])\n    label_encoders[col] = le\n\n# Standardize numerical features\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(features)\n\n# Convert scaled features back to a DataFrame for clarity\nscaled_features_df = pd.DataFrame(scaled_features, columns=features.columns)\n\n# Display the prepared data\nscaled_features_df.head()\n\n\n\n\n\n\n\n\n\nIncome\nPolicy Type\nNumber of Open Complaints\nVehicle Class\nTotal Claim Amount\navg_sentiment_score\navg_sentiment\nMonthly Premium Auto\n\n\n\n\n0\n0.313373\n-1.820746\n3.177325\n-0.883569\n-0.517532\n-0.587768\n-0.690552\n-0.688114\n\n\n1\n-1.250543\n0.356568\n-0.407649\n0.554023\n1.236955\n-1.149417\n-0.690552\n0.574745\n\n\n2\n0.698166\n0.356568\n-0.407649\n0.554023\n0.467334\n0.557206\n1.346743\n1.100936\n\n\n3\n0.244109\n0.356568\n-0.407649\n1.512417\n-0.276365\n-0.587768\n-0.690552\n-0.540780\n\n\n4\n-1.250543\n0.356568\n-0.407649\n-0.883569\n0.072253\n-1.149417\n-0.690552\n-0.835447\n\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\n\n# Determine the optimal number of clusters using the Elbow Method\ninertia = []\nrange_n_clusters = range(2, 11)\n\nfor k in range_n_clusters:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(scaled_features)\n    inertia.append(kmeans.inertia_)\n\n# Plot the Elbow Method\nplt.figure(figsize=(8, 6))\nplt.plot(range_n_clusters, inertia, marker='o', linestyle='--')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia')\nplt.title('Elbow Method for Optimal k')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate Silhouette Scores for each cluster count\nsilhouette_scores = []\n\nfor k in range_n_clusters:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    cluster_labels = kmeans.fit_predict(scaled_features)\n    score = silhouette_score(scaled_features, cluster_labels)\n    silhouette_scores.append(score)\n\n# Plot Silhouette Scores\nplt.figure(figsize=(8, 6))\nplt.plot(range_n_clusters, silhouette_scores, marker='o', linestyle='--')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Scores for Different k')\nplt.show()\n\n# Optimal number of clusters based on Silhouette Scores\noptimal_k = range_n_clusters[silhouette_scores.index(max(silhouette_scores))]\nprint(\"The best cluster number is:\", optimal_k)\n\n\n\n\n\n\n\n\n\nThe best cluster number is: 8\n\n\nHere, we use the elbow method and calculate the silhouette scores for each cluster count. The silhouette score measures how well each point fits within its assigned cluster. Higher scores indicate better-defined clusters. Based on the data visualization and highest Silhouette Score, k=8 provides the most cohesive and separated clusters in this analysis.\n\n\nCode\nfrom sklearn.manifold import TSNE\n\n# Perform K-Means clustering with the optimal number of clusters (k=2)\nkmeans = KMeans(n_clusters=optimal_k, random_state=42)\ncluster_labels = kmeans.fit_predict(scaled_features)\n\n# Add cluster labels to the original scaled features for visualization\nscaled_features_df['Cluster'] = cluster_labels\n\n# Use t-SNE to reduce data to 2D for visualization\ntsne = TSNE(n_components=2, random_state=42)\ntsne_results = tsne.fit_transform(scaled_features)\n\n# Visualize clusters in 2D space\nplt.figure(figsize=(8, 6))\nplt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=cluster_labels, cmap='viridis', s=50)\nplt.colorbar(label='Cluster')\nplt.xlabel('t-SNE Dimension 1')\nplt.ylabel('t-SNE Dimension 2')\nplt.title('t-SNE Visualization of Clusters')\nplt.show()\n\n\n\n\n\n\n\n\n\nPerform k-means clustering with the optimal number of clusters. The clusters are generally well-separated, indicating that the K-Means algorithm successfully grouped data points with similar characteristics. There are areas of tight clustering (yellow cluster in the bottom) where data points are highly similar. Then we use dimension reduction techniques t-SNE to reduce the dimensions to two dimensions. The yellow cluster at the bottom might represent data points with strong similarity in features like sentiment, claim amount, or premium.\n\n\nCode\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n# data['Cluster'] = cluster_labels\n\n# # Dive deeper into specific attributes by comparing clusters\n# attributes_to_explore = ['Income', 'Policy Type', 'Number of Open Complaints', \n#                  'Vehicle Class', 'Total Claim Amount', \n#                  'avg_sentiment_score', 'avg_sentiment', 'Monthly Premium Auto']\n\n# # Create boxplots to compare attribute distributions across clusters\n# for attribute in attributes_to_explore:\n#     plt.figure(figsize=(8, 6))\n#     sns.boxplot(x=\"Cluster\", y=attribute, data=data, palette=\"viridis\")\n#     plt.title(f\"Distribution of {attribute} Across Clusters\")\n#     plt.xlabel(\"Cluster\")\n#     plt.ylabel(attribute)\n#     plt.show()\n\n# # Visualize the relationship between sentiment score and claim amount for both clusters\n# plt.figure(figsize=(8, 6))\n# sns.scatterplot(\n#     x=\"avg_sentiment_score\",\n#     y=\"Total Claim Amount\",\n#     hue=\"Cluster\",\n#     data=data,\n#     palette=\"viridis\",\n#     alpha=0.7\n# )\n# plt.title(\"Sentiment Score vs Total Claim Amount by Cluster\")\n# plt.xlabel(\"Average Sentiment Score\")\n# plt.ylabel(\"Total Claim Amount\")\n# plt.legend(title=\"Cluster\")\n# plt.show()\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata['Cluster'] = cluster_labels\n\nattributes_to_explore = ['Income', 'Total Claim Amount', 'Number of Open Complaints', \n                        'Total Claim Amount', 'avg_sentiment_score', 'Monthly Premium Auto']\n\nfor attribute in attributes_to_explore:\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x=\"Cluster\", y=attribute, data=data, palette=\"viridis\", showfliers=False, width=0.3)\n    sns.stripplot(x=\"Cluster\", y=attribute, data=data, jitter=True, alpha=0.6, palette=\"viridis\")\n    plt.title(f\"Density Boxplot of {attribute} Across Clusters\", fontsize=14)\n    plt.xlabel(\"Cluster\", fontsize=12)\n    plt.ylabel(attribute, fontsize=12)\n    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n    plt.tight_layout()\n    plt.show()\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    x=\"avg_sentiment_score\",\n    y=\"Total Claim Amount\",\n    hue=\"Cluster\",\n    data=data,\n    palette=\"viridis\",\n    alpha=0.7\n)\nplt.title(\"Sentiment Score vs Total Claim Amount by Cluster\", fontsize=14)\nplt.xlabel(\"Average Sentiment Score\", fontsize=12)\nplt.ylabel(\"Total Claim Amount\", fontsize=12)\nplt.legend(title=\"Cluster\", fontsize=10)\nplt.grid(axis=\"both\", linestyle=\"--\", alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe scatterplot visualizes the relationship between average sentiment score and total claim amount, with points color-coded by cluster. This plot helps identify patterns, such as how sentiment relates to claim amounts for different clusters.\n\nInterpretations\nThis analysis successfully segmented the customer base into 8 distinct clusters, offering valuable insights into customer behavior, sentiment, income, and claims patterns. By employing K-means clustering with k=8, the study identified cohesive and well-separated groups, each characterized by unique attributes and tendencies. The use of t-SNE further enabled effective visualization of these clusters, highlighting key differences and overlaps.\nThe findings revealed important correlations, such as the preference of wealthier customers for comprehensive insurance policies and the strong relationship between sentiment and claims behavior. Specifically, customers with positive sentiment tend to have lower claim amounts, reflecting higher satisfaction and fewer complaints, while negative sentiment correlates with higher claims and potential dissatisfaction. These observations underline the importance of monitoring customer sentiment as a predictive factor for claims and overall satisfaction."
  },
  {
    "objectID": "lagged_ml.html",
    "href": "lagged_ml.html",
    "title": "Lagged Sentiment Clustering",
    "section": "",
    "text": "Code\n%matplotlib inline\nCode\nimport plotly.offline as pyo\npyo.init_notebook_mode(connected=True)\nIn today’s competitive insurance market, customer satisfaction is a critical factor for retaining clients and fostering loyalty. Dissatisfaction often arises from unmet expectations, complex policies, and perceived lack of value. By identifying key drivers of customer sentiment, such as customer lifetime value, monthly premium auto payment, and policy complexity, companies can implement targeted strategies to address these issues. This report explores actionable recommendations for improving customer service, simplifying processes, and enhancing transparency, ultimately leading to a more positive customer experience and stronger long-term relationships.\nThis analysis investigates customer segmentation using lagged sentiment data from the second half of 2023, combined with external insurance-related data from the corresponding period in 2024. By aligning these datasets, we aim to explore whether past customer sentiments can predict or relate to future customer behavior and insurance choices. The goal is to determine if trends in lagged sentiments foreshadow changes in customer satisfaction, engagement, or risk, offering actionable insights for strategic decision-making in the insurance industry."
  },
  {
    "objectID": "lagged_ml.html#data-munging",
    "href": "lagged_ml.html#data-munging",
    "title": "Lagged Sentiment Clustering",
    "section": "Data Munging",
    "text": "Data Munging\n\n\nCode\nimport pandas as pd\n\n# Read a Parquet file\ndf = pd.read_csv('../../data/cleaned_data/lagged_second_half_merged.csv')\npd.set_option('display.max_columns', None)\n\n# Display the DataFrame\ndf.head()\n \n\n\n\n\n\n\n\n\n\nUnnamed: 0\nCustomer\nstate\nCustomer Lifetime Value\nResponse\nCoverage\nCoverage Index\nEducation\nEducation Index\nEmployment Status\nEmployment Status Index\nGender\nIncome\nLocation\nLocation Index\nMarital Status\nMarital Status Index\nMonthly Premium Auto\nMonths Since Last Claim\nMonths Since Policy Inception\nNumber of Open Complaints\nNumber of Policies\nPolicy Type\nPolicy Type Index\nPolicy\nPolicy Index\nRenew Offer Type\nSales Channel\nSales Channel Index\nTotal Claim Amount\nVehicle Class\nVehicle Class Index\nVehicle Size\nVehicle Size Index\nmonth\nyear\nlagged_avg_sentiment_score\nlagged_avg_sentiment\n\n\n\n\n0\n1\nSJ95423\nArizona\n11905.68\nYes\nBasic\n0\nHigh School or Below\n0\nEmployed\n1\nM\n134791\nSuburban\n1\nMarried\n1\n149\n31\n34\n1\n8\nCorporate Auto\n1\nCorporate L3\n5\n2\nBranch\n1\n712.80\nSUV\n4\nMedsize\n1\n10\n2024\n-0.275080\nNegative\n\n\n1\n2\nDP45816\nArizona\n15712.20\nNo\nExtended\n1\nHigh School or Below\n0\nEmployed\n1\nF\n83006\nUrban\n2\nMarried\n1\n131\n7\n88\n0\n2\nPersonal Auto\n0\nPersonal L3\n2\n4\nBranch\n1\n628.10\nTwo-Door Car\n0\nSmall\n0\n10\n2024\n-0.275080\nNegative\n\n\n2\n3\nIC13702\nCalifornia\n9407.26\nNo\nBasic\n0\nBachelor\n2\nUnemployed\n0\nF\n0\nSuburban\n1\nSingle\n0\n93\n45\n82\n1\n2\nPersonal Auto\n0\nPersonal L3\n2\n1\nAgent\n2\n670.68\nFour-Door Car\n1\nSmall\n0\n10\n2024\n-0.032152\nNeutral\n\n\n3\n4\nJT52858\nOregon\n12187.62\nNo\nPremium\n2\nBachelor\n2\nEmployed\n1\nF\n133653\nSuburban\n1\nMarried\n1\n151\n45\n101\n1\n8\nPersonal Auto\n0\nPersonal L1\n0\n3\nBranch\n1\n725.76\nFour-Door Car\n1\nMedsize\n1\n10\n2024\n-0.050936\nNegative\n\n\n4\n5\nCE56187\nOregon\n5890.22\nNo\nBasic\n0\nMaster\n3\nEmployed\n1\nF\n73594\nUrban\n2\nDivorced\n2\n147\n7\n49\n3\n1\nPersonal Auto\n0\nPersonal L2\n1\n2\nBranch\n1\n386.42\nSUV\n4\nMedsize\n1\n10\n2024\n-0.050936\nNegative\n\n\n\n\n\n\n\n\nCorrelation Matrix\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.manifold import TSNE\n\n\n\n\nCode\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\n\n\n\nCode\n# Compute the correlation matrix\ncorrelation_matrix = df.corr(method='spearman')\nmask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n\n# Create a heatmap\nplt.figure(figsize=(14, 12))\nsns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFiltering Out Variables with Low Variance\n\n\nCode\n# Check standard deviation\nlow_variance_cols = df.std()[df.std() &lt; 0.3].index\n\n\n\n\nCode\nprint(\"Low variance columns:\", low_variance_cols)\n\n\nLow variance columns: Index(['year', 'lagged_avg_sentiment_score'], dtype='object')\n\n\n\n\nFeature Selection\n\n\nCode\ndf_numeric = df.select_dtypes(include=['float64', 'int64'])\nscaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df_numeric)\n\nX = df_numeric.drop(['Unnamed: 0', 'lagged_avg_sentiment_score', 'year', 'month'], axis=1)\ny = df['lagged_avg_sentiment']\n\n# Encode categorical target variable\nle = LabelEncoder()\ny = le.fit_transform(y)\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Initialize the model\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Train the model\nrf.fit(X_train_scaled, y_train)\n\ny_pred = rf.predict(X_test_scaled)\n\n# Feature importance from the model\nimportances = rf.feature_importances_\nfeatures = X.columns\n\n# Plot feature importance\nplt.figure(figsize=(10, 6))\nplt.barh(features, importances, color='skyblue')\nplt.xlabel('Importance Score')\nplt.title('Feature Importance')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Feature importance and corresponding features\nfeature_importances = {\n'Customer Lifetime Value': 0.1078197162706497,\n'Coverage Index': 0.021983766542722275,\n'Education Index': 0.0438022231745862,\n'Employment Status Index': 0.021578314932309814,\n'Income': 0.08533880836102012,\n'Location Index': 0.02057917406542304,\n'Marital Status Index': 0.026878443379556395,\n'Monthly Premium Auto': 0.08902222517414524,\n'Months Since Last Claim': 0.09424962486220208,\n'Months Since Policy Inception': 0.10292679555348216,\n'Number of Open Complaints': 0.026825888624876497,\n'Number of Policies': 0.0434765546161667,\n'Policy Type Index': 0.017613337550058984,\n'Policy Index': 0.05412930771195091,\n'Renew Offer Type': 0.03774035239650668,\n'Sales Channel Index': 0.041876868596344895,\n'Total Claim Amount': 0.10764063070424754,\n'Vehicle Class Index': 0.030534956323217705,\n'Vehicle Size Index': 0.02598301116053306\n}\n\n\n\n\nCode\n# Convert to a pandas DataFrame for easier manipulation\ndf_importance = pd.DataFrame(list(feature_importances.items()), columns=['Feature', 'Importance'])\n\n# Sort by importance\ndf_importance = df_importance.sort_values(by='Importance', ascending=False)\n\n# Choose top N features (let's select the top 10 for example)\nselected_features = df_importance['Feature'].head(10).values\nprint(selected_features)\n\n\n['month' 'state_encoded' 'Customer Lifetime Value' 'Total Claim Amount'\n 'Months Since Policy Inception' 'Months Since Last Claim'\n 'Monthly Premium Auto' 'Income' 'Policy Index' 'Number of Policies']\n\n\n\n\nDistributions of the Variables by Cluster\n\n\nCode\nattributes_num = ['Income', 'Customer Lifetime Value', 'Total Claim Amount', 'Monthly Premium Auto', 'lagged_avg_sentiment_score']\nattributes_cat = ['Policy', 'Education', 'Vehicle Class', 'Marital Status']\n\nattributes_to_explore = ['Income', 'Policy Index', 'Education Index', 'Customer Lifetime Value', 'Marital Status Index',\n                         'lagged_avg_sentiment_score', 'Total Claim Amount', 'Monthly Premium Auto', 'Vehicle Class Index']\n\nkmeans = KMeans(n_clusters=10, random_state=42)\ndf[\"Temp_Cluster\"] = kmeans.fit_predict(df[attributes_to_explore])\n\n# Create density boxplots with jitter for each attribute across clusters\nfor attribute in attributes_num:\n    plt.figure(figsize=(10, 6))\n    \n    # Boxplot without outliers\n    sns.boxplot(x=\"Temp_Cluster\", y=attribute, data=df, palette=\"viridis\", showfliers=False, width=0.3)\n    \n    # Add jittered points for density\n    sns.stripplot(x=\"Temp_Cluster\", y=attribute, data=df, jitter=True, alpha=0.6, palette=\"viridis\")\n    \n    plt.title(f\"Density Boxplot of {attribute} Across Clusters\", fontsize=14)\n    plt.xlabel(\"Temp_Cluster\", fontsize=12)\n    plt.ylabel(attribute, fontsize=12)\n    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n    plt.tight_layout()\n    plt.show()\n\n# Plot the bar graph\nfor attribute in attributes_cat:\n    cluster_counts = df.groupby(['Temp_Cluster', attribute]).size().reset_index(name='count')\n    \n    plt.figure(figsize=(10, 6))\n    sns.barplot(\n        data=cluster_counts,  # Use the grouped data\n        x='Temp_Cluster',\n        y='count',\n        hue=attribute,\n        palette='Set2'\n    )\n\n    # Customize the plot\n    plt.title(f'Distribution of {attribute} Across Clusters', fontsize=16)\n    plt.xlabel('Temp_Cluster', fontsize=12)\n    plt.ylabel('Count', fontsize=12)\n    plt.legend(title=attribute, fontsize=10)\n    plt.xticks(fontsize=10)\n    plt.yticks(fontsize=10)\n    plt.tight_layout()\n\n    # Show the plot\n    plt.show()"
  },
  {
    "objectID": "lagged_ml.html#k-means-clustering-using-t_sne",
    "href": "lagged_ml.html#k-means-clustering-using-t_sne",
    "title": "Lagged Sentiment Clustering",
    "section": "K-Means Clustering using t_SNE",
    "text": "K-Means Clustering using t_SNE\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.manifold import TSNE\n\n# Sample selected features\nselected_features = ['lagged_avg_sentiment_score', 'Customer Lifetime Value', 'Income', 'Monthly Premium Auto']\n\n# Assuming df is your original DataFrame\nX = df[selected_features]\n\n# Standardize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Range of K to try (convert to a list)\nk_values = list(range(2, 11))\n\nsil_scores = []\n\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X_scaled)\n    \n    # Calculate silhouette score\n    score = silhouette_score(X_scaled, kmeans.labels_)\n    sil_scores.append(score)\n\n# Create figure for the subplots\nfig, ax = plt.subplots(1, 3, figsize=(18, 6))\n\n# Plot silhouette scores in the first subplot\nax[0].plot(k_values, sil_scores, marker='o', linestyle='-', color='b')\nax[0].set_title(\"Silhouette Score vs. Number of Clusters (K)\", fontsize=14)\nax[0].set_xlabel(\"Number of Clusters (K)\", fontsize=12)\nax[0].set_ylabel(\"Silhouette Score\", fontsize=12)\nax[0].grid(True)\n\n# Find the optimal K with the highest silhouette score\noptimal_k = k_values[sil_scores.index(max(sil_scores))]\nprint(f'Optimal K: {optimal_k}')\n\n# Perform K-Means clustering for optimal_k\nkmeans = KMeans(n_clusters=optimal_k, random_state=42)\nclusters = kmeans.fit_predict(X_scaled)\n\n# Add cluster labels to data\ndf['Cluster'] = clusters\n\n# Perform t-SNE for the optimal K\ntsne = TSNE(n_components=2, random_state=42)\nX_embedded = tsne.fit_transform(X_scaled)\n\n# Add t-SNE results to the dataframe\ndf['t-SNE_1'] = X_embedded[:, 0]\ndf['t-SNE_2'] = X_embedded[:, 1]\n\n# Compute centroids in t-SNE space\ncentroids = df.groupby('Cluster')[['t-SNE_1', 't-SNE_2']].mean()\n\n# Plot t-SNE visualization for optimal K (second subplot)\nscatter = ax[1].scatter(df['t-SNE_1'], df['t-SNE_2'], c=df['Cluster'], cmap='viridis', s=20)\nax[1].scatter(centroids['t-SNE_1'], centroids['t-SNE_2'], marker='x', color='red', s=100, label=\"Centroids\")\nax[1].set_title(\"Cluster Visualization with t-SNE (Optimal K)\", fontsize=14)\nax[1].legend()\n\n# Perform K-Means clustering for k=5\nkmeans = KMeans(n_clusters=5, random_state=42)\nclusters = kmeans.fit_predict(X_scaled)\n\n# Add cluster labels to data\ndf['Cluster'] = clusters\n\n# Perform t-SNE for k=5\nX_embedded = tsne.fit_transform(X_scaled)\n\n# Add t-SNE results to the dataframe\ndf['t-SNE_1'] = X_embedded[:, 0]\ndf['t-SNE_2'] = X_embedded[:, 1]\n\n# Compute centroids in t-SNE space\ncentroids = df.groupby('Cluster')[['t-SNE_1', 't-SNE_2']].mean()\n\n# Plot t-SNE visualization for k=5 (third subplot)\nscatter = ax[2].scatter(df['t-SNE_1'], df['t-SNE_2'], c=df['Cluster'], cmap='viridis', s=20)\nax[2].scatter(centroids['t-SNE_1'], centroids['t-SNE_2'], marker='x', color='red', s=100, label=\"Centroids\")\nax[2].set_title(\"Cluster Visualization with t-SNE (k=5)\", fontsize=14)\nax[2].legend()\n\n# Adjust the layout to make the plot larger horizontally\nplt.tight_layout()\nplt.show()\n\n\nOptimal K: 2\n\n\n\n\n\n\n\n\n\nI used the Silhouette score to determine the optimal number of clusters, identifying ( k=2 ) as the best choice. To visualize the clusters, I applied t-SNE for dimensionality reduction and plotted the clusters for both ( k=2 ) and a larger ( k=5 ). The t-SNE plots revealed that the clusters were not clearly separable. Additionally, upon examining the variables contributing to cluster formation, I found that lagged_avg_sentiment_score was not a significant factor. Since the goal was to explore the relationship between lagged_avg_sentiment_score and other variables, I adjusted the clustering approach to prioritize this variable and considered a larger ( k ) for better-defined clusters. Finally, I examined the correlations between lagged_avg_sentiment_score and variables such as Customer Lifetime Value, Income, and Monthly Premium Auto within each cluster, which revealed relationships between sentiment scores and these variables.\n\n\nCode\ndf_numeric = df[['lagged_avg_sentiment_score', 'Customer Lifetime Value', 'Income', 'Monthly Premium Auto', 'Cluster']]\n\nfor cluster in df_numeric['Cluster'].unique():\n    print(f\"Cluster {cluster}:\")\n    cluster_data = df_numeric[df_numeric['Cluster'] == cluster]\n    print(cluster_data.corr()['lagged_avg_sentiment_score'])\n\n\nCluster 4:\nlagged_avg_sentiment_score    1.000000\nCustomer Lifetime Value       0.014924\nIncome                       -0.039060\nMonthly Premium Auto          0.000466\nCluster                            NaN\nName: lagged_avg_sentiment_score, dtype: float64\nCluster 0:\nlagged_avg_sentiment_score    1.000000\nCustomer Lifetime Value       0.013525\nIncome                       -0.063500\nMonthly Premium Auto          0.040972\nCluster                            NaN\nName: lagged_avg_sentiment_score, dtype: float64\nCluster 3:\nlagged_avg_sentiment_score    1.000000\nCustomer Lifetime Value      -0.084488\nIncome                       -0.049478\nMonthly Premium Auto         -0.077793\nCluster                            NaN\nName: lagged_avg_sentiment_score, dtype: float64\nCluster 1:\nlagged_avg_sentiment_score    1.000000\nCustomer Lifetime Value      -0.128715\nIncome                       -0.073325\nMonthly Premium Auto         -0.000165\nCluster                            NaN\nName: lagged_avg_sentiment_score, dtype: float64\nCluster 2:\nlagged_avg_sentiment_score    1.000000\nCustomer Lifetime Value      -0.121950\nIncome                       -0.072261\nMonthly Premium Auto         -0.199451\nCluster                            NaN\nName: lagged_avg_sentiment_score, dtype: float64\n\n\nThe correlations between lagged_avg_sentiment_score and other variables across clusters show consistent but weak relationships. In particular, sentiment scores exhibit small negative correlations with Income across all clusters, suggesting that individuals with lower incomes may tend to have more positive sentiment scores. Similarly, weak negative correlations with Customer Lifetime Value and Monthly Premium Auto are observed in some clusters, with a slightly stronger negative trend for Monthly Premium Auto in Cluster 2. Overall, these weak correlations suggest that sentiment is only minimally influenced by these variables, and any trends may vary depending on the cluster."
  },
  {
    "objectID": "lagged_ml.html#interpretations",
    "href": "lagged_ml.html#interpretations",
    "title": "Lagged Sentiment Clustering",
    "section": "Interpretations",
    "text": "Interpretations\nThe analysis of lagged sentiment data reveals meaningful patterns in customer segmentation, with Customer Lifetime Value and Monthly Premium Auto emerging as significant drivers of segmentation. Customers with higher Customer Lifetime Value, higher Monthly Premium Auto payments, more comprehensive or expensive policies, and marital status tend to exhibit lower sentiment scores. This suggests that dissatisfaction among high-value customers may persist over time, potentially influencing their behavior and risk profiles in 2024. These findings highlight the predictive power of lagged sentiment data in anticipating shifts in customer satisfaction and engagement.\nIn contrast, customers with lower incomes display slightly higher sentiment scores, likely due to modest expectations and perceived value from more affordable policies and services. The lagged period (2023 sentiment data vs. 2024 behavior) could indicate that lower-income customers maintain consistent sentiment over time due to less fluctuation in policy complexity or cost burdens, which tend to impact higher-income segments more prominently.\nThese insights emphasize the value of analyzing historical sentiment data to proactively identify dissatisfaction and address emerging risks. Insurance companies can leverage these findings to implement tailored interventions, such as simplifying policies, offering loyalty rewards, and enhancing transparency. By addressing these issues, companies can improve customer satisfaction, mitigate future dissatisfaction, and foster stronger relationships with their customers."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sentiment-Driven Customer Segmentation in Auto Insurance",
    "section": "",
    "text": "This project aims to explore customer sentiment in the auto insurance industry by leveraging Reddit discussions and external customer data. By combining lagged and current sentiment data with customer attributes, the goal is to identify patterns and relationships that influence customer behavior and satisfaction. Through clustering analysis, we aim to segment customers based on their sentiment and associated features such as financial, policy-related, and demographic variables. These insights can help businesses better understand customer needs, improve policy offerings, and enhance overall customer experience. The analysis includes exploratory data analysis (EDA), natural language processing (NLP) for sentiment extraction, and machine learning (ML) techniques to cluster customers effectively."
  },
  {
    "objectID": "index.html#primary-research-questions",
    "href": "index.html#primary-research-questions",
    "title": "Sentiment-Driven Customer Segmentation in Auto Insurance",
    "section": "Primary Research Questions",
    "text": "Primary Research Questions\n\n\n\n\n\n\nQuestion 1\n\n\n\nHow do current customer sentiments correlate with external auto insurance attributes to identify distinct customer segments?\n\n\n\nUsing the current_first_half_merged.csv dataset, this question focuses on current customer sentiments, which represent Reddit sentiments from January 2024 to June 2024 merged with external auto insurance data from the same period.\n\n\n\n\n\n\nQuestion 2\n\n\n\nHow do past customer sentiments (lagged sentiments) predict or relate to customer segments based on their later behavior and insurance choices?\n\n\n\nUsing lagged customer sentiments, this question investigates past customer sentiments from June 2023 to December 2023 (from Reddit) and their relationship with auto insurance data for June 2024 to December 2024. This analysis involves exploring how lagged Reddit sentiment trends align with subsequent customer behavior and decisions."
  },
  {
    "objectID": "index.html#secondary-research-questions",
    "href": "index.html#secondary-research-questions",
    "title": "Sentiment-Driven Customer Segmentation in Auto Insurance",
    "section": "Secondary Research Questions",
    "text": "Secondary Research Questions\n\n\n\n\n\n\nQuestion\n\n\n\nHow do sentiment trends vary across states over time from June 2023 to July 2024?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAre there specific months with significant sentiment shifts across states?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the distribution of sentiment categories (Negative, Neutral, Positive, Strong Negative, Strong Positive) across states?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAre certain states more prone to negative or positive sentiment categories than others?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhich state has the highest percentage of positive sentiment?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow does positive sentiment vary among states, and what insights can be drawn about state-wise sentiment disparities?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow do marital status groups differ in terms of average premium, claim amount, and income?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDoes marital status influence the number of customers and financial metrics in the dataset?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhich state has the highest total claim amount, and how does it compare to other states?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow do state-wise claim amounts correlate with customer satisfaction or sentiment?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow do vehicle class preferences vary by marital status and customer lifetime value?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhich marital status group represents the highest customer lifetime value in different vehicle classes?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAre high-value customers concentrated in specific vehicle classes or marital groups?"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This exploratory data analysis (EDA) aims to uncover insights from two combined datasets—one containing lagged sentiment data from the second half of the previous year and the other with current sentiment data from the first half of this year. By integrating customer data with sentiment scores, this analysis seeks to explore how customer sentiment correlates with key variables such as Customer Lifetime Value, coverage types, financial metrics, and policy details across different states and time periods. Through the lens of five key research quesßtions, the analysis will identify trends, patterns, and relationships to provide a deeper understanding of the interplay between customer sentiment and various attributes, paving the way for actionable insights.\n\n\n\n\n\n\nQuestion\n\n\n\nHow do state-wise sentiment trends change over time within the selected time periods?\n\n\n\n\n\n\n\n\nThe plot illustrates the sentiment trends for five states over the period from June 2023 to July 2024. Nevada consistently exhibits the most negative sentiment, particularly in October and November 2023, where its sentiment scores are significantly lower than other states. In contrast, California tends to show relatively positive sentiment throughout the timeline, with peaks in July 2023 and smaller variations compared to other states. Arizona shows fluctuating sentiment, occasionally trending upwards to positive values in February 2024. States like Oregon and Washington exhibit moderately negative sentiment, though Washington stands out with occasional positive spikes, such as in October 2023. Overall, the months of October and November 2023 demonstrate downward sentiment trends across most states, while February 2024 shows improvement for several states, such as Nevada and Arizona. Similar patterns are seen between Oregon and Washington, which generally move in the same direction during several months.\n\n\n\n\n\n\nThis table above summarizes the distribution of sentiment categories (Negative, Neutral, Positive, Strong Negative, Strong Positive) for each state based on auto insurance-related sentiment data. California, Oregon, and Washington have the highest counts of negative sentiments, implying potential dissatisfaction in these regions. Nevada is the only state with presence of strong positive sentiments, but it is also the state with the presense of highest negative. This suggests that Nevada auto insurance sentiments tend to change rapidly and aggressively from one month to another. Oregon and Arizona exhibit relatively balanced counts among the categories, with more positive sentiments than strong extremes. Overall, all five states have higher average negative sentiments for each month within the year than the positive ones, which suggests all states more dissatisfied auto insurance related customers than the satisfied ones. When aggregating the above table\n\n\n\n\n\n\nThe above shows the percentage of positive sentiment among Reddit commenters across five states. While all five states predominantly exhibit negative sentiment overall, Nevada stands out with the highest positive sentiment at 45.45%, suggesting relatively more favorable discussions. In contrast, California has the lowest positive sentiment at just 9.09%, indicating limited positivity in online conversations.\n\n\n\n\nSummary of Customer Metrics by Marital Status\n\n\nMarital.Status\navg_premium\nnumber_of_customers\navg_claim_amount\navg_income\n\n\n\n\nDivorced\n127.30\n524\n556.37\n57178.83\n\n\nMarried\n125.80\n2153\n512.37\n60943.99\n\n\nSingle\n124.89\n990\n745.57\n30615.05\n\n\n\n\n\n\n\n\nThis table summarizes customer metrics by marital status:\n\nDivorced: Customers pay the highest average premium ($127.30) and have moderate claim amounts ($556.37) and income ($57,178.83).\nMarried: Customers have the highest number of customers (2,153), the lowest average claim amount ($512.37), and the highest income ($60,943.99).\nSingle: Customers pay the lowest average premium ($124.89), have the highest average claim amount ($745.57), and the lowest income ($30,615.05).\n\nThis suggests that marital status significantly influences these financial metrics.\n\n\n\nTotal Claim Amount by State\n\n\nState\ntotal_claim_amount\n\n\n\n\nArizona\n414801.2\n\n\nCalifornia\n739033.1\n\n\nNevada\n193150.1\n\n\nOregon\n580152.2\n\n\nWashington\n205650.9\n\n\n\n\n\n\n\nThe table above shows that California has the highest total claim amount at $739,033.1, followed by Oregon at $580,152.2. Nevada and Washington have relatively lower claim amounts, with Arizona being mid-range among the states.\n\nThe chart shows the distribution of Customer Lifetime Value across vehicle classes and marital status. “Four-Door Cars” and SUVs are leading the customer preference, dominated by married customers who also represent a larger share of higher CLV tiers. Single customers are more equitably distributed across vehicle classes but less concentrated in high-value segments, while divorced customers have the smallest representation overall, with fewer high-value customers. Low and medium categories are leading most of the vehicle classes, which shows an opportunity to increase the high-value customers. The businesses can be targeted towards married customers in the high-value vehicle categories of “Four-Door Cars” and SUVs while exploring strategies for engaging and upselling single and divorced customers in niche categories such as “Luxury Cars” and “Luxury SUVs.”\n\n\nCode\n# Create a 'month_year' column for easy grouping\nreddit_df['month_year'] = pd.to_datetime(reddit_df[['year', 'month']].assign(day=1))\n\n# Group by subreddit, sentiment, and month_year, then count the occurrences\nsentiment_counts = reddit_df.groupby(['subreddit', 'general_sentiment', 'month_year']).size().reset_index(name='count')\n\n# Map sentiment to indices for Sankey diagram\nsentiment_mapping = {\n    'Strong Positive': 0,\n    'Positive': 1,\n    'Neutral': 2,\n    'Negative': 3,\n    'Strong Negative': 4\n}\n\n# Prepare the source, target, and values for the Sankey diagram\nsources = []\ntargets = []\nvalues = []\nlinks_colors = []\nlabels = ['Strong Positive', 'Positive', 'Neutral', 'Negative', 'Strong Negative']\n\n# Define color map for different sentiment flows (links)\nsentiment_colors = {\n    'Strong Positive': '#6baed6',  # Soft Blue\n    'Positive': '#66c2a5',         # Light Green\n    'Neutral': '#f7c09e',          # Soft Yellow\n    'Negative': '#f58484',         # Light Coral\n    'Strong Negative': '#c4a6d2'   # Lavender\n}\n\n# Create a list of subreddits\nsubreddits = sentiment_counts['subreddit'].unique()\nfor idx, subreddit in enumerate(subreddits):\n    # Add nodes for subreddits\n    labels.append(subreddit)\n    \n    # Iterate through sentiments\n    for sentiment, sentiment_idx in sentiment_mapping.items():\n        filtered_data = sentiment_counts[(sentiment_counts['subreddit'] == subreddit) &\n                                         (sentiment_counts['general_sentiment'] == sentiment)]\n        \n        for _, row in filtered_data.iterrows():\n            sources.append(sentiment_idx)\n            targets.append(len(sentiment_mapping) + idx)  # Subreddit node index\n            values.append(row['count'])\n            links_colors.append(sentiment_colors[sentiment])\n\n# Create Sankey diagram\nfig = go.Figure(go.Sankey(\n    node=dict(\n        pad=15,\n        thickness=20,\n        line=dict(color=\"black\", width=0.5),\n        label=labels,\n        color=\"lightgray\"\n    ),\n    link=dict(\n        source=sources,\n        target=targets,\n        value=values,\n        color=links_colors\n    )\n))\n\nfig.update_layout(title_text=\"Sentiment Flow Across Subreddits\", font_size=10)\nfig.show()\n\n\n\n\nWarning: `includeHTML()` was provided a `path` that appears to be a complete HTML document.\n✖ Path: /Users/parsakeyvani/Downloads/reddit_viz1.html\nℹ Use `tags$iframe()` to include an HTML document. You can either ensure `path` is accessible in your app or document (see e.g. `shiny::addResourcePath()`) and pass the relative path to the `src` argument. Or you can read the contents of `path` and pass the contents to `srcdoc`.\n\n\n\n\n\n                            \n                                            \n\n\n\n\nThis Sankey diagram visualizes the flow of sentiments across three subreddits—cars, legaladvice, and personalfinance—from June 2023 to July 2024. The diagram is divided into five sentiment categories: Strong Positive, Positive, Neutral, Negative, and Strong Negative. Legaladvice consistently exhibits a high volume of Strong Negative sentiment, reflecting that many users in this subreddit are seeking help for difficult or negative situations. Despite the predominance of negative sentiment, there are fluctuations in Positive and Neutral sentiments, suggesting that while a significant portion of discussions is focused on negative experiences, there are also moments of positive or neutral advice shared.\nIn the Personalfinance subreddit, Strong Positive sentiment takes up a larger share, indicating that conversations are primarily centered around positive financial experiences and achievements. However, there also exists negative sentiments, revealing that not all discussions in this subreddit are positive. The Cars subreddit shows relatively lower activity compared to the other two, suggesting limited engagement in the topic.\nThis plot offers valuable insights into how sentiments evolve for each subreddit during the time period, helping to identify the most prominent sentiment categories and trends throughout the year.\n\n\nCode\nfrom plotly.subplots import make_subplots\n\n# Create a 'month_year' column for easy grouping\nreddit_df['month_year'] = pd.to_datetime(reddit_df[['year', 'month']].assign(day=1))\n\n# Group by 'month_year', 'subreddit', and 'general_sentiment', and count occurrences\nsentiment_counts = reddit_df.groupby(['month_year', 'subreddit', 'general_sentiment']).size().reset_index(name='count')\n\n# Pivot data to have sentiment types as columns for each month\npivot_data = sentiment_counts.pivot_table(index=['month_year', 'subreddit'], columns='general_sentiment', values='count', aggfunc='sum', fill_value=0)\n\n# Reset index for easy manipulation\npivot_data = pivot_data.reset_index()\n\n# Calculate total counts by each subreddit (sum across all sentiments and months)\ntotal_counts = pivot_data.groupby('subreddit').sum().sum(axis=1)\n\n# Normalize the counts by dividing the counts for each sentiment and month by the total count for each subreddit\nfor sentiment in pivot_data.columns[2:]:  # Start from the sentiment columns (ignore 'month_year' and 'subreddit')\n    pivot_data[sentiment] = pivot_data.apply(\n        lambda row: row[sentiment] / total_counts[row['subreddit']] if total_counts[row['subreddit']] &gt; 0 else 0, axis=1\n    )\n\n# Number of months (should be the same for each subreddit)\nnum_months = len(pivot_data['month_year'].unique())\n\n# Define angles (uniformly distributed around the circle)\nangles = np.linspace(0.0, 2 * np.pi, num_months, endpoint=False)\n\n# Define the sentiment types\nsentiments = ['Strong Positive', 'Positive', 'Neutral', 'Negative', 'Strong Negative']\n\n# Define a custom color palette for each subreddit\nsubreddit_colors = {\n    'cars': '#6baed6',  # Soft Blue\n    'legaladvice': '#66c2a5',         # Light Green\n    'personalfinance': '#f7c09e'     # Soft Yellow\n}\n\n# Create subplots: 5 subplots, each for one sentiment type\nfig = make_subplots(\n    rows=1, cols=5,  # 1 row, 5 columns\n    subplot_titles=sentiments,\n    shared_yaxes=True,  # Share the y-axis across subplots\n    horizontal_spacing=0.09,\n    specs=[[{'type': 'polar'}]*5]  # Specify polar subplot type\n)\n\n# Loop over sentiment types and plot each one in its own subplot\nfor idx, sentiment in enumerate(sentiments):\n    # Filter data for the current sentiment\n    data = pivot_data[['month_year', 'subreddit', sentiment]]\n    \n    # Ensure the data for months is aligned with the number of months\n    for subreddit in pivot_data['subreddit'].unique():\n        sentiment_data = data[data['subreddit'] == subreddit][sentiment].values\n        if sentiment_data.shape[0] &lt; num_months:  # In case some months are missing for the sentiment\n            sentiment_data = np.pad(sentiment_data, (0, num_months - sentiment_data.shape[0]), constant_values=0)\n        \n        # Plot sentiment data for the current sentiment and subreddit\n        fig.add_trace(\n            go.Scatterpolar(\n                r=sentiment_data,\n                theta=data['month_year'].dt.strftime('%b %Y').unique(),\n                mode='lines',\n                name=subreddit,\n                line=dict(color=subreddit_colors.get(subreddit, '#333333'), width=3),  # Default color if subreddit not in the palette\n                hovertemplate=(\n                    f'Sentiment: {sentiment}&lt;br&gt;'  # Display the sentiment type\n                    f'Subreddit: {subreddit}&lt;br&gt;'  # Display the subreddit\n                    'Count: %{r}&lt;br&gt;'  # Display the sentiment count (r)\n                    '&lt;extra&gt;&lt;/extra&gt;'  # Hide the default trace name\n                )\n            ),\n            row=1, col=idx+1  # Place each trace in the corresponding subplot\n        )\n\n# Update layout and axis settings\nfig.update_layout(\n    title='Sentiment Trends Across Subreddits (June 2023 - July 2024)',\n    showlegend=True,\n    legend=dict(\n        x=1.05,  # Move the legend to the right outside the plot\n        y=0.5,   # Center the legend vertically\n        traceorder='normal',\n        font=dict(size=12),\n        borderwidth=1,\n        bordercolor='Black'\n    ),\n    height=430,  # Set plot height\n    width=1800,  # Set plot width\n)\n\n# Loop over each subplot to individually set the radial axis range for all subplots\nfor i in range(1, 6):  # Loop through all 5 subplots (polar subplots)\n    fig.update_layout(\n        **{\n            f'polar{i}': {\n                'radialaxis': {\n                    'range': [0, 0.07],  # Set the same range for each subplot\n                    'showticklabels': True,\n                    'ticks': ''\n                }\n            }\n        }\n    )\n\n# Show the plot\nfig.show()\n\n\n\n\nWarning: `includeHTML()` was provided a `path` that appears to be a complete HTML document.\n✖ Path: /Users/parsakeyvani/Downloads/reddit_viz2.html\nℹ Use `tags$iframe()` to include an HTML document. You can either ensure `path` is accessible in your app or document (see e.g. `shiny::addResourcePath()`) and pass the relative path to the `src` argument. Or you can read the contents of `path` and pass the contents to `srcdoc`.\n\n\n\n\n\n                            \n                                            \n\n\n\n\nTo gain a deeper understanding of sentiment trends across subreddits, we calculated the normalized sentiment counts for each subreddit and month. This normalization helped us compare the sentiment distributions across subreddits by adjusting for differences in total activity levels. Even after normalization, we observed that LegalAdvice exhibited a consistent dominance of strong negative sentiments, suggesting that discussions within this subreddit tend to be more focused on challenging or negative legal situations.\nInterestingly, the Cars subreddit showed a more diverse range of sentiments, with strong positive, positive, and negative sentiments being notably present. This suggests the existence of two distinct groups within the community—those who are satisfied with their cars and those who are dissatisfied. These variations in sentiment could be influenced by factors such as the type of vehicle, the model, or individual experiences related to car ownership.\nOverall, the results suggest that while conversations around LegalAdvice and PersonalFinance tend to maintain a steady sentiment, discussions about Cars reflect a broader emotional spectrum, possibly due to varying personal experiences related to vehicle ownership.\n\n\nCode\n# Group the data by 'state' and 'general_sentiment' to get the average sentiment and count\nstate_sentiment = reddit_df.groupby(['state', 'general_sentiment']).agg(\n    post_count=('state', 'size'),\n    avg_sentiment=('compound_score', 'mean')\n).reset_index()\n\n# Create a bubble chart\nfig = px.scatter(state_sentiment, \n                 x='state', \n                 y='avg_sentiment', \n                 size='post_count', \n                 color='general_sentiment',\n                 hover_name='state',\n                 size_max=100,\n                 title='Bubble Chart: State vs. Average Sentiment by Sentiment Type')\n\nfig.update_layout(\n    xaxis_title=\"State\",\n    yaxis_title=\"Average Sentiment Score\",\n    legend_title=\"Sentiment\",\n    showlegend=True,\n    width=1000,\n    height=600\n)\n\nfig.show()\n\n\n\n\nWarning: `includeHTML()` was provided a `path` that appears to be a complete HTML document.\n✖ Path: /Users/parsakeyvani/Downloads/reddit_viz3.html\nℹ Use `tags$iframe()` to include an HTML document. You can either ensure `path` is accessible in your app or document (see e.g. `shiny::addResourcePath()`) and pass the relative path to the `src` argument. Or you can read the contents of `path` and pass the contents to `srcdoc`.\n\n\n\n\n\n                            \n                                            \n\n\n\n\nIn this bubble chart, we explore whether certain states have a higher number of posts or comments associated with specific sentiments, aiming to identify if any state stands out with a particular sentiment that is notably prevalent. What we observed is that California had the largest number of posts and comments across all sentiment categories, indicating that this state has a significantly higher volume of activity compared to others. This results in an imbalance in the number of posts and comments across the states, with California’s dominance clearly visible. The chart also reveals that most of the posts in California are associated with Strong Negative and Strong Positive sentiments, suggesting that discussions in this state tend to involve extreme emotional tones, whether positive or negative.\nWhen looking at other states, we notice that Strong Negative sentiment is the most prevalent sentiment. This indicates that in all of the five states, the conversations are largely focused on negative experiences or challenging situations. In contrast to California, where strong positive and negative sentiments are more balanced, other states seem to lean more towards negative sentiments.\nOverall, the plot provides a clear visualization of how sentiments are distributed across different states, with notable trends such as California’s high volume of extreme sentiments and the dominance of Strong Negative sentiment in most other states. This allows us to gain valuable insights into how different states engage with various emotional tones in online discussions throughout the year.\n\n\nCode\n# Plot the choropleth map for the selected year\nfig = px.choropleth(\n    state_monthly_sentiment, \n    locations='state_abb',\n    locationmode='USA-states',\n    color='compound_score', \n    color_continuous_scale=\"teal\",\n    hover_name='state_abb',\n    animation_frame='year_month',  # This will animate through each month\n    labels={'compound_score': 'Average Sentiment Score'},\n    title=\"Monthly Sentiment Score by State (2023 & 2024)\"\n)\n\nfig.update_geos(\n    visible=True,\n    showlakes=True,\n    lakecolor=\"rgb(255, 255, 255)\",\n    projection_type=\"albers usa\"\n)\n\nfig.show()\n\n\n\n\nWarning: `includeHTML()` was provided a `path` that appears to be a complete HTML document.\n✖ Path: /Users/parsakeyvani/Downloads/reddit_viz4.html\nℹ Use `tags$iframe()` to include an HTML document. You can either ensure `path` is accessible in your app or document (see e.g. `shiny::addResourcePath()`) and pass the relative path to the `src` argument. Or you can read the contents of `path` and pass the contents to `srcdoc`.\n\n\n\n\n\n                            \n                                            \n\n\n\n\nLastly, we wanted to examine how sentiment scores varied across different states over time. To achieve this, we used a geoplot to visualize sentiment distribution by state, showing how sentiment shifted from one month to another. This plot provides a geographical view of sentiment scores across states, allowing us to identify regional trends and how they evolved over the year. By observing the colors representing sentiment intensities and changes over time, we can track fluctuations in sentiment across different areas, revealing patterns of positive, neutral, and negative sentiment across the country.\nFrom the geoplot, we can see that California consistently exhibits high levels of Positive sentiment, particularly in the summer months, with a noticeable peak in June 2024. This trend suggests that conversations in California are generally more positive, especially around events or topics that likely resonate with residents during this time. Meanwhile, Nevada shows a more mixed sentiment distribution, with both Neutral and Negative sentiments appearing prominently throughout the year. Interestingly, Strong Negative sentiment rises slightly during the winter months, possibly reflecting seasonal or local events. In Oregon, Negative sentiment appears more frequently compared to other states, with Neutral sentiments fluctuating, especially around November 2023. The emotional tone in Oregon seems more balanced, with less drastic swings between positive and negative sentiments. Washington displays a slightly higher amount of Neutral sentiment, indicating that posts from this state are generally more balanced or indifferent in tone. The Strong Negative sentiment in Washington is notably low, suggesting fewer extreme negative emotions in the discourse compared to the other states. Finally, Arizona shows more Neutral sentiment across the year, with occasional spikes in Strong Positive sentiment, particularly in April 2024. The state’s sentiment distribution appears more stable, with fewer fluctuations in extreme emotional tones."
  },
  {
    "objectID": "images/joe-eda(1).html",
    "href": "images/joe-eda(1).html",
    "title": "Group 25 Final Project",
    "section": "",
    "text": "!pip install seaborn\n\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.11/site-packages (0.13.2)\nRequirement already satisfied: numpy!=1.24.0,&gt;=1.20 in /opt/conda/lib/python3.11/site-packages (from seaborn) (1.26.4)\nRequirement already satisfied: pandas&gt;=1.2 in /opt/conda/lib/python3.11/site-packages (from seaborn) (2.2.3)\nRequirement already satisfied: matplotlib!=3.6.1,&gt;=3.4 in /opt/conda/lib/python3.11/site-packages (from seaborn) (3.9.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (1.3.0)\nRequirement already satisfied: cycler&gt;=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (4.54.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (1.4.7)\nRequirement already satisfied: packaging&gt;=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (24.1)\nRequirement already satisfied: pillow&gt;=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (10.4.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (3.2.0)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (2.9.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3)\nRequirement already satisfied: tzdata&gt;=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2024.2)\nRequirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (1.16.0)\n\n\n\nimport seaborn as sns\n\nfacet_data = data.copy()\nfacet_data['Customer Lifetime Value'] = pd.qcut(\n    facet_data['Customer Lifetime Value'], 4, labels=['Low', 'Medium', 'High', 'Very High']\n)\n\n# multi-faceted plot\nplt.figure(figsize=(24, 20))\nsns.catplot(\n    data=facet_data,\n    x='Vehicle Class',\n    hue='Customer Lifetime Value',\n    col='Marital Status',\n    kind='count',\n    height=6,\n    aspect=1.5,\n    palette='coolwarm'\n)\nplt.subplots_adjust(top=0.9)\nplt.suptitle('Breakdown of Customer Lifetime Value by Vehicle Class and Marital Status')\nplt.show()\n\n&lt;Figure size 2400x2000 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n# Setup - Run only once per Kernel App\n%conda install https://anaconda.org/conda-forge/openjdk/11.0.1/download/linux-64/openjdk-11.0.1-hacce0ff_1021.tar.bz2\n\n# install PySpark\n%pip install pyspark==3.4.0\n\n# restart kernel\nfrom IPython.core.display import HTML\nHTML(\"&lt;script&gt;Jupyter.notebook.kernel.restart()&lt;/script&gt;\")\n\n\nDownloading and Extracting Packages:\n\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: pyspark==3.4.0 in /opt/conda/lib/python3.11/site-packages (3.4.0)\nRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark==3.4.0) (0.10.9.7)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\n\n\n# Import pyspark and build Spark session\nfrom pyspark.sql import SparkSession\n\nspark = (\n    SparkSession.builder.appName(\"PySparkApp\")\n    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n    .config(\n        \"fs.s3a.aws.credentials.provider\",\n        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n    )\n    .getOrCreate()\n)\n\nprint(spark.version)\n\nWarning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\nIvy Default Cache set to: /home/sagemaker-user/.ivy2/cache\nThe jars for the packages stored in: /home/sagemaker-user/.ivy2/jars\norg.apache.hadoop#hadoop-aws added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-2f79cd19-de81-48ae-aae5-fba9107879f6;1.0\n    confs: [default]\n    found org.apache.hadoop#hadoop-aws;3.2.2 in central\n    found com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n:: resolution report :: resolve 210ms :: artifacts dl 7ms\n    :: modules in use:\n    com.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n    org.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n    ---------------------------------------------------------------------\n    |                  |            modules            ||   artifacts   |\n    |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n    ---------------------------------------------------------------------\n    |      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n    ---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-2f79cd19-de81-48ae-aae5-fba9107879f6\n    confs: [default]\n    0 artifacts copied, 2 already retrieved (0kB/10ms)\n24/12/10 05:12:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/12/10 05:12:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n\n\n:: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n3.4.0\n\n\n\nfrom pyspark.sql import SparkSession\n\n# Create a SparkSession\nspark = SparkSession.builder \\\n    .appName(\"Insurance Data Analysis\") \\\n    .getOrCreate()\n\n# Read the CSV file with semicolon delimiter\ndf_spark = spark.read \\\n    .option(\"delimiter\", \";\") \\\n    .option(\"header\", \"true\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .csv('../../data/cleaned_data/external_data_cleaned.csv')\n\n# Show the schema and first few rows\ndf_spark.printSchema()\ndf_spark.show(5)\n\nroot\n |-- \"\",\"Customer\",\"State\",\"Customer Lifetime Value\",\"Response\",\"Coverage\",\"Coverage Index\",\"Education\",\"Education Index\",\"Effective To Date\",\"Employment Status\",\"Employment Status Index\",\"Gender\",\"Income\",\"Location\",\"Location Index\",\"Marital Status\",\"Marital Status Index\",\"Monthly Premium Auto\",\"Months Since Last Claim\",\"Months Since Policy Inception\",\"Number of Open Complaints\",\"Number of Policies\",\"Policy Type\",\"Policy Type Index\",\"Policy\",\"Policy Index\",\"Renew Offer Type\",\"Sales Channel\",\"Sales Channel Index\",\"Total Claim Amount\",\"Vehicle Class\",\"Vehicle Class Index\",\"Vehicle Size\",\"Vehicle Size Index\": string (nullable = true)\n\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|\"\",\"Customer\",\"State\",\"Customer Lifetime Value\",\"Response\",\"Coverage\",\"Coverage Index\",\"Education\",\"Education Index\",\"Effective To Date\",\"Employment Status\",\"Employment Status Index\",\"Gender\",\"Income\",\"Location\",\"Location Index\",\"Marital Status\",\"Marital Status Index\",\"Monthly Premium Auto\",\"Months Since Last Claim\",\"Months Since Policy Inception\",\"Number of Open Complaints\",\"Number of Policies\",\"Policy Type\",\"Policy Type Index\",\"Policy\",\"Policy Index\",\"Renew Offer Type\",\"Sales Channel\",\"Sales Channel Index\",\"Total Claim Amount\",\"Vehicle Class\",\"Vehicle Class Index\",\"Vehicle Size\",\"Vehicle Size Index\"|\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \"1\",\"QC35222\",\"Ca...|\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \"2\",\"AE98193\",\"Wa...|\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \"3\",\"TM23514\",\"Or...|\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \"4\",\"WB38524\",\"Ca...|\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \"5\",\"QZ42725\",\"Wa...|\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 5 rows\n\n\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, avg, count\n\nspark = SparkSession.builder.appName(\"SummaryTable\").getOrCreate()\n\ndata_path = \"../../data/cleaned_data/external_data_cleaned.csv\"  # Replace with the correct path\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(data_path)\n\ndf.printSchema()\n\nsummary_table = (\n    df.groupBy(\"Marital Status\")\n    .agg(\n        avg(\"Monthly Premium Auto\").alias(\"avg_premium\"),\n        count(\"Customer\").alias(\"number_of_customers\"),\n        avg(\"Total Claim Amount\").alias(\"avg_claim_amount\"),\n        avg(\"Income\").alias(\"avg_income\")\n    )\n    .orderBy(\"Marital Status\")\n)\n\nsummary_table.show()\n\nroot\n |-- _c0: integer (nullable = true)\n |-- Customer: string (nullable = true)\n |-- State: string (nullable = true)\n |-- Customer Lifetime Value: double (nullable = true)\n |-- Response: string (nullable = true)\n |-- Coverage: string (nullable = true)\n |-- Coverage Index: integer (nullable = true)\n |-- Education: string (nullable = true)\n |-- Education Index: integer (nullable = true)\n |-- Effective To Date: string (nullable = true)\n |-- Employment Status: string (nullable = true)\n |-- Employment Status Index: integer (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Income: integer (nullable = true)\n |-- Location: string (nullable = true)\n |-- Location Index: integer (nullable = true)\n |-- Marital Status: string (nullable = true)\n |-- Marital Status Index: integer (nullable = true)\n |-- Monthly Premium Auto: integer (nullable = true)\n |-- Months Since Last Claim: integer (nullable = true)\n |-- Months Since Policy Inception: integer (nullable = true)\n |-- Number of Open Complaints: integer (nullable = true)\n |-- Number of Policies: integer (nullable = true)\n |-- Policy Type: string (nullable = true)\n |-- Policy Type Index: integer (nullable = true)\n |-- Policy: string (nullable = true)\n |-- Policy Index: integer (nullable = true)\n |-- Renew Offer Type: integer (nullable = true)\n |-- Sales Channel: string (nullable = true)\n |-- Sales Channel Index: integer (nullable = true)\n |-- Total Claim Amount: double (nullable = true)\n |-- Vehicle Class: string (nullable = true)\n |-- Vehicle Class Index: integer (nullable = true)\n |-- Vehicle Size: string (nullable = true)\n |-- Vehicle Size Index: integer (nullable = true)\n\n+--------------+------------------+-------------------+-----------------+------------------+\n|Marital Status|       avg_premium|number_of_customers| avg_claim_amount|        avg_income|\n+--------------+------------------+-------------------+-----------------+------------------+\n|      Divorced|127.30152671755725|                524|556.3712022900759|57178.833969465646|\n|       Married|125.79516953088714|               2153|512.3705759405486| 60943.98699489085|\n|        Single|124.88585858585859|                990|745.5708989899006|30615.054545454546|\n+--------------+------------------+-------------------+-----------------+------------------+\n\n\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\n\nspark = SparkSession.builder.appName(\"TotalClaimAmountByState\").getOrCreate()\n\ndata_path = \"../../data/cleaned_data/external_data_cleaned.csv\"\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(data_path)\n\ntotal_claims_by_state = (\n    df.groupBy(\"State\")\n    .agg(\n        F.sum(\"Total Claim Amount\").alias(\"total_claim_amount\")\n    )\n    .orderBy(\"State\")\n)\n\ntotal_claims_by_state.show()\n\n24/12/10 05:16:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n\n\n+----------+------------------+\n|     State|total_claim_amount|\n+----------+------------------+\n|   Arizona|414801.25000000006|\n|California| 739033.1200000007|\n|    Nevada|193150.09999999995|\n|    Oregon| 580152.1700000012|\n|Washington|205650.90999999995|\n+----------+------------------+\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfacet_data = data.copy()\n\nfacet_data['Customer Lifetime Value'] = pd.qcut(\n    facet_data['Customer Lifetime Value'], 4, labels=['Low', 'Medium', 'High', 'Very High']\n)\n\nplt.figure(figsize=(10, 15))\nsns.catplot(\n    data=facet_data,\n    x='Vehicle Class',\n    hue='Customer Lifetime Value',\n    col='Marital Status',\n    kind='count',\n    col_wrap=1,\n    height=5,\n    aspect=1.5,\n    palette='coolwarm'\n)\n\nplt.subplots_adjust(top=0.9)\nplt.suptitle('Breakdown of Customer Lifetime Value by Vehicle Class and Marital Status')\nplt.show()\n\n&lt;Figure size 1000x1500 with 0 Axes&gt;"
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "This section outlines the approach taken to process and analyze Reddit data from selected subreddits, focusing on discussions involving auto and insurance topics. The process involved selecting relevant subreddits, filtering posts based on specific keywords and geographic locations, cleaning and preparing text data, and merging it with external datasets for advanced analysis. After preprocessing, the cleaned data comprised 1,523 observations and 12 variables.\n\nSubreddit Selection and Filtering\nWe began with data from five subreddits: personalfinance, legaladvice, cars, insurance, and insuranceclaims. Using criteria specific to the analysis, we reduced this list to the final three subreddits:\n\nlegaladvice\npersonalfinance\ncars\n\nThe filtering was justified based on the relevance of discussions to auto and insurance topics. Posts that did not meet these criteria (e.g., discussions unrelated to auto or insurance) were excluded.\n\n\nFiltering by Keywords and Geographic Criteria\nWithin each subreddit, submissions and comments were filtered for relevance. Posts and comments were retained if they:\n\nContained keywords related to auto (e.g., “car,” “vehicle”) or insurance (e.g., “insurance,” “claims”).\nReferenced specific U.S. states: Arizona, California, Nevada, Oregon, and Washington.\n\nBelow is the filtering code used for this step:\n\nfrom pyspark.sql.functions import col, when, regexp_extract\n\n# Define states and keywords\nstates = [\"Arizona\", \"California\", \"Nevada\", \"Oregon\", \"Washington\"]\nkeywords = [\"car\", \"vehicle\", \"auto\", \"insurance\", \"claims\"]\n\n# Filter Submissions\nfiltered_submissions = df_submissions.filter(\n    col(\"subreddit\").isin([\"legaladvice\", \"personalfinance\", \"cars\", \"insuranceclaims\"]) &\n    (col(\"title\").rlike(\"|\".join(keywords)) | col(\"selftext\").rlike(\"|\".join(keywords))) &\n    (col(\"title\").rlike(\"|\".join(states)) | col(\"selftext\").rlike(\"|\".join(states)))\n)\n\n# Extract state information\nfiltered_submissions = filtered_submissions.withColumn(\n    \"state\", when(col(\"title\").rlike(\"|\".join(states)), regexp_extract(col(\"title\"), \"|\".join(states), 0))\n    .otherwise(regexp_extract(col(\"selftext\"), \"|\".join(states), 0))\n)\n\n# Combine titles and selftext into a single column\nfiltered_submissions = filtered_submissions.withColumn(\n    \"body\", concat_ws(\" \", col(\"title\"), col(\"selftext\"))\n).select(\"subreddit\", \"body\", \"state\")\n\nfiltered_submissions.show(5)\n\n\n\nText Preprocessing and Cleaning\nThe text data underwent several preprocessing steps to prepare it for Natural Language Processing (NLP). The steps included:\n\nCleaning: Removal of special characters, extra spaces, and numerical values.\nTokenization: Splitting text into individual words.\nStopword Removal: Eliminating common but irrelevant words like “the” and “is.”\nLemmatization: Converting words to their root forms (e.g., “running” → “run”).\nNormalization: Converting text to lowercase and standardizing word forms.\n\nBelow is the text preprocessing code:\n\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nimport spacy\n\n# Load spaCy language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Define preprocessing function\ndef preprocess_text(text):\n    if text is None:\n        return \"\"\n    text = re.sub(r'[^A-Za-z\\s]', '', str(text))  # Remove special characters\n    text = re.sub(r'\\s+', ' ', text).strip()      # Remove extra spaces\n    doc = nlp(text.lower())                      # Tokenize and lowercase\n    tokens = [token.lemma_ for token in doc if not token.is_stop]  # Lemmatize and remove stopwords\n    return ' '.join(tokens)\n\n# Register UDF for preprocessing\npreprocess_udf = udf(preprocess_text, StringType())\n\n# Apply preprocessing to text column\nprocessed_data = filtered_submissions.withColumn(\"cleaned_body\", preprocess_udf(col(\"body\")))\n\nprocessed_data.show(5)\n\n\n\nNatural Language Processing (NLP) Analysis\n\nSentiment Analysis: VADER (Valence Aware Dictionary and sEntiment Reasoner) was applied to determine the sentiment of posts and comments, categorizing them as Positive, Neutral, or Negative based on a compound score.\n\n\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\nanalyzer = SentimentIntensityAnalyzer()\n\ndef vader_sentiment(text):\n    if not text:\n        return 0.0\n    return analyzer.polarity_scores(text)[\"compound\"]\n\n# Apply sentiment scoring\nsentiment_udf = udf(vader_sentiment, FloatType())\nsentiment_data = processed_data.withColumn(\"compound_score\", sentiment_udf(col(\"cleaned_body\")))\nsentiment_data.show(5)\n\n\nTF-IDF (Term Frequency-Inverse Document Frequency): Important words were identified by analyzing their relevance using TF-IDF scores.\n\n\nfrom pyspark.ml.feature import Tokenizer, HashingTF, IDF\nfrom pyspark.ml import Pipeline\n\ntokenizer = Tokenizer(inputCol=\"cleaned_body\", outputCol=\"words\")\nhashing_tf = HashingTF(inputCol=\"words\", outputCol=\"raw_features\", numFeatures=5000)\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\npipeline = Pipeline(stages=[tokenizer, hashing_tf, idf])\ntfidf_model = pipeline.fit(processed_data)\ntfidf_data = tfidf_model.transform(processed_data)\n\ntfidf_data.select(\"cleaned_body\", \"features\").show(5)\n\n\n\n\n\n\n\n\n\nMerging with External Data\nThe merging of the Reddit data with the external auto insurance dataset was carried out in R, ensuring alignment across relevant temporal and geographic criteria. This step was critical to augmenting Reddit discussions with meaningful external insights, such as claims trends and insurance rates by state and time period. Two merged datasets were created to address different analytical scenarios: Current First Half Sentiment and Lagged Previous Second Half Sentiment.\n\nPrepare External Data: The external data was filtered to exclude records from 2011 and transformed to include month and year variables extracted from the Effective To Date field.\n\n\nlibrary(lubridate)\nlibrary(dplyr)\n\n# Clean and transform external data\nexternal_data_filtered &lt;- data %&gt;%\n  mutate(\n    month = month(mdy(`Effective To Date`)),  # Parse \"Month/Day/Year\" format\n    year = year(mdy(`Effective To Date`))    # Extract year\n  ) %&gt;%\n  select(-`Effective To Date`) %&gt;%\n  rename(state = State) %&gt;%\n  filter(year != 2011)  # Exclude records from 2011\n\n\nAggregate Reddit Data: The Reddit data was grouped by month, year, and state, calculating average sentiment scores and classifications for each grouping.\n\n\n# Aggregate Reddit data\nreddit_data_aggregated2 &lt;- reddit_data %&gt;%\n  group_by(month, year, state) %&gt;%\n  summarise(average_sent_score = mean(compound_score)) %&gt;%\n  mutate(\n    average_sentiment = case_when(\n      average_sent_score &gt;= 0.7 ~ \"Strong Positive\",\n      average_sent_score &gt;= 0.05 & average_sent_score &lt; 0.7 ~ \"Positive\",\n      average_sent_score &gt; -0.05 & average_sent_score &lt; 0.05 ~ \"Neutral\",\n      average_sent_score &lt;= -0.05 & average_sent_score &gt; -0.7 ~ \"Negative\",\n      average_sent_score &lt;= -0.7 ~ \"Strong Negative\",\n      TRUE ~ \"Undefined\"\n    )\n  )\n\n\n\nCreate Merged Datasets:\n\nFirst Half Sentiment (1/24 - 6/24): This dataset merged external data from January to June 2024 with Reddit data from the same period.\nLagged Previous Second Half Sentiment (6/23 - 12/23 with 6/24 - 12/24): This dataset merged external data from June to December 2024 with Reddit data from the lagged period of June to December 2023.\n\n\n\n# First Half Sentiment\nfirst_half_sentiment &lt;- external_data_filtered %&gt;%\n  filter(year == 2024 & month &gt;= 1 & month &lt;= 6) %&gt;%\n  left_join(\n    reddit_data_aggregated2 %&gt;%\n      filter(year == 2024 & month &gt;= 1 & month &lt;= 6),\n    by = c(\"month\", \"year\", \"state\")\n  ) %&gt;%\n  rename(\n    avg_sentiment_score = average_sent_score,\n    avg_sentiment = average_sentiment\n  )\n\n# Lagged Previous Second Half Sentiment\nlagged_previous_second_half_sentiment &lt;- external_data_filtered %&gt;%\n  filter(year == 2024 & month &gt;= 6 & month &lt;= 12) %&gt;%\n  left_join(\n    reddit_data_aggregated2 %&gt;%\n      filter(year == 2023 & month &gt;= 6 & month &lt;= 12),\n    by = c(\"month\", \"state\")  # Match on month and state, ignoring year\n  ) %&gt;%\n  select(-year.y) %&gt;%\n  rename(\n    year = year.x,\n    lagged_avg_sentiment_score = average_sent_score,\n    lagged_avg_sentiment = average_sentiment\n  )\n\nThe resulting datasets were saved as CSV files for further EDA visualizations and ML clustering\n\n# Save merged datasets\nwrite.csv(first_half_sentiment, \"/Users/parsakeyvani/Downloads/big_data_project/Big_data_cleaned_datasets/current_first_half_merged.csv\")\nwrite.csv(lagged_previous_second_half_sentiment, \"/Users/parsakeyvani/Downloads/big_data_project/Big_data_cleaned_datasets/lagged_second_half_merged.csv\")"
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "This project successfully addressed two key questions about customer sentiment in the auto insurance domain using clustering analysis. In the Current Sentiment Clustering, we identified that wealthier customers tend to prefer comprehensive policies, and positive sentiments correlate with lower claim amounts, indicating higher satisfaction and fewer complaints. Conversely, negative sentiment is linked to higher claim amounts, highlighting dissatisfaction. These findings underscore the value of real-time sentiment monitoring for predicting claims and enhancing customer satisfaction.\nIn the Lagged Sentiment Clustering, we explored the relationship between past customer sentiment (2023 data) and future behaviors (2024 data). We found that customers with higher Customer Lifetime Value, Monthly Premium Auto payments, and more comprehensive policies exhibit lower sentiment scores, suggesting dissatisfaction among high-value customers. Additionally, lower-income customers displayed slightly higher sentiment scores, likely reflecting modest expectations and perceived value from simpler policies. These insights demonstrated the potential of lagged sentiment data for predicting customer satisfaction trends and identifying emerging risks.\nThrough this project, we gained valuable experience in integrating external data with Reddit sentiment analysis and leveraging NLP and clustering techniques for actionable insights. For future work, we propose focusing on more specific aspects of auto insurance sentiment, such as claims or premiums, instead of general auto insurnace sentiments. Expanding the dataset to include additional subreddits and extending the time period beyond one year could provide richer data for state-level analyses and enable more precise sentiment predictions for external variables. This approach could further refine our understanding of customer satisfaction and enhance predictive accuracy in the auto insurance industry."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]